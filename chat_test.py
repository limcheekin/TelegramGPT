import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock
from chat import ChatManager, ChatContext, ChatState, ChatData
from models import Conversation, UserMessage, AssistantMessage
from db import Database # Assuming DB needs mocking or a test instance
from gemini import GPTClient # Or the specific client being used
from chat import TELEGRAM_MAX_MESSAGE_LENGTH

# --- Test Setup Helpers (adapt as needed) ---

# Mock Telegram Bot
def create_mock_bot():
    mock_bot = MagicMock(spec=['send_message', 'edit_message_text', 'delete_message'])
    mock_bot.send_message = AsyncMock(return_value=MagicMock(id=5678)) # Mock sent message ID
    mock_bot.edit_message_text = AsyncMock()
    mock_bot.delete_message = AsyncMock()
    return mock_bot

# Mock DB
def create_mock_db():
    mock_db = MagicMock(spec=Database)
    mock_db.add_message = AsyncMock()
    mock_db.update_message = AsyncMock()
    mock_db.update_conversation = AsyncMock()
    # Add other methods as needed by the tested code
    return mock_db

# Mock LLM Client Stream
async def mock_llm_stream(content_chunks): # content_chunks should be a list of AssistantMessage objects
    for chunk_message in content_chunks: # Iterate through the AssistantMessage objects
        # Mock the structure expected from gemini.py's __stream -> complete
        yield chunk_message # Yield the AssistantMessage object directly
        await asyncio.sleep(0.01) # Simulate network delay

# Mock GPTClient
def create_mock_gpt_client(content_chunks):
    mock_gpt = MagicMock(spec=GPTClient)
    # Make the 'complete' method return our async generator
    mock_gpt.complete = MagicMock(return_value=mock_llm_stream(content_chunks))
    return mock_gpt

# Mock ChatContext/State
def create_chat_context(chat_id=123):
    # Simplified state/context for this test
    chat_state = ChatState()
    chat_data = ChatData(conversations={}, modes={}, current_mode_id=None)
    return ChatContext(chat_id, chat_state, chat_data)

# --- Test Case ---

@pytest.mark.asyncio
async def test_complete_message_exceeding_length_limit_in_chunks():
    """
    Verify that message length check correctly identifies long messages
    accumulated from multiple small chunks and appends 'continue' text.
    """
    CHAT_ID = 123
    USER_MSG_ID = 100
    SENT_MSG_ID = 5678 # Matches mock_bot.send_message return

    # Simulate chunks that individually are small but cumulatively exceed limit
    long_content_part1 = "A" * 2500
    long_content_part2 = "B" * 2000
    full_accumulated_content = long_content_part1 + long_content_part2
    total_length = len(full_accumulated_content)

    # Ensure the test setup still makes sense (total > limit)
    assert total_length > TELEGRAM_MAX_MESSAGE_LENGTH

    # Accumulate content as it *should* be yielded by the generator in chat.py
    # This mock needs to yield AssistantMessage objects containing the *cumulative* content
    # based on how the gemini.py client was written
    accumulated_chunk_messages = []
    current_content = ""
    for part in [long_content_part1, long_content_part2]:
         current_content += part
         # Assume the yielded object has the full content so far
         # Use a placeholder ID for the assistant message generated by the LLM mock
         mock_assistant_msg = AssistantMessage(id=SENT_MSG_ID, content=current_content, replied_to_id=USER_MSG_ID)
         accumulated_chunk_messages.append(mock_assistant_msg)


    mock_bot = create_mock_bot()
    mock_db = create_mock_db()
    # Adjust mock_gpt_client to yield AssistantMessage objects
    mock_gpt = create_mock_gpt_client(accumulated_chunk_messages) # Pass message objects
    chat_context = create_chat_context(CHAT_ID)

    # Create a dummy conversation and user message
    user_message = UserMessage(id=USER_MSG_ID, content="Test message")
    conversation = Conversation(id=1, title=None, started_at=user_message.timestamp, messages=[user_message])
    # Set current conversation *before* calling __complete
    chat_context.chat_state.current_conversation = conversation


    chat_manager = ChatManager(
        gpt=mock_gpt,
        speech=None,
        bot=mock_bot,
        context=chat_context,
        conversation_timeout=None,
        db=mock_db
    )

    # --- Act ---
    # Call the internal __complete method directly for focused testing
    await chat_manager._ChatManager__complete(conversation, SENT_MSG_ID)

    # --- Assert ---
    # 1. Check intermediate edits (optional but good)
    assert mock_bot.edit_message_text.call_count >= 2 # Edits for chunks + final edit

    # 2. Check the *final* edit call
    final_call_args = mock_bot.edit_message_text.call_args_list[-1]
    args, kwargs = final_call_args

    # **** FIX IS HERE: Construct expected_final_text using the SAME logic ****
    # Calculate the expected truncated content
    truncation_point = TELEGRAM_MAX_MESSAGE_LENGTH - 100
    expected_truncated_content = full_accumulated_content[:truncation_point]

    # Build the expected final text EXACTLY as the code does
    expected_final_text = f"{expected_truncated_content}...\n\n(Please type \"continue\" to view the rest of the message.)"
    # ***********************************************************************

    assert kwargs.get('chat_id') == CHAT_ID
    assert kwargs.get('message_id') == SENT_MSG_ID
    # Now compare the actual text with the correctly calculated expected text
    assert kwargs.get('text') == expected_final_text

    # 3. Verify DB interactions (optional)
    # Check that the DB was updated with the *truncated* text
    # Get the last call to update_message
    update_calls = mock_db.update_message.call_args_list
    assert len(update_calls) >= 1
    last_update_args, last_update_kwargs = update_calls[-1]
    # Arguments are (message_id, new_content)
    assert last_update_args[0] == SENT_MSG_ID # Check correct message ID
    # Check that the content saved matches the truncated text sent to the user
    assert last_update_args[1] == expected_final_text #<-- Verify DB got truncated text
